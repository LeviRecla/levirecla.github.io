<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="description" content="NLP Projects by Levi Recla" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>NLP Projects - Levi Recla</title>
  <link
    href="https://fonts.googleapis.com/css2?family=Inter:wght@300;500;700&display=swap"
    rel="stylesheet"
  />
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <header class="header-bar">
    <div class="header-logo">
      <a href="index.html">Levi Recla</a>
    </div>
    <nav class="header-nav">
      <a href="about.html">About</a>
      <a href="education.html">Education</a>
      <a href="experience.html">Experience</a>
      <a href="projects.html">Projects</a>
      <a href="nlp.html">NLP</a>
      <a href="skills.html">Skills</a>
      <a href="contact.html">Contact</a>
    </nav>
  </header>

  <section class="content-section">
    <h1>Natural Language Processing (NLP)</h1>

    <div class="chat-bubble">
      <p>
        My work in NLP explores the intersection of language, code, and scientific computation.
        I focus on applying modern language models to automate programming language translation, understand grounded semantics, and improve scientific data interpretation.
        This page highlights key NLP projects, research contributions, and model development work.
      </p>
    </div>

    <!-- LANGUAGE MODEL PRETRAINING SECTION -->
<div class="language-section">
    <h2><i class="fas fa-robot"></i> Language Model Pretraining</h2>
    <div class="chat-bubble">
      <h3>Pretraining a Transformer Language Model from Scratch</h3>
      <ul>
        <li>Pretrained a language model on a custom code dataset using a tokenizer trained from scratch with HuggingFace's `tokenizers` library.</li>
        <li>Used `Trainer` API from HuggingFace Transformers to manage the full training loop, metrics, and model saving.</li>
        <li>Pretraining was conducted using a causal language modeling objective (`AutoModelForCausalLM`) with attention to loss curves and training efficiency.</li>
        <li>Configured custom training arguments including batch size, evaluation strategy, learning rate, and warm-up steps.</li>
        <li>Model checkpoints and loss plots were analyzed to monitor progress and verify stability during training.</li>
        <li>Check it out here: <a href="https://github.com/LeviRecla/Language-Model-Pre-Training" target="_blank" class="github-button">
          <i class="fas fa-file-code"></i>View Notebook</a></li>
      </ul>
    </div>
  </div>
  

    <!-- Project 1 -->
    <div class="chat-bubble">
      <h2>Fine-Tuning a Language Model for Fortran-to-Python Translation</h2>
      <ul>
        <li>Fine-tuned StarCoder-3B on manually aligned Fortran–Python code pairs.</li>
        <li>Achieved a 4× improvement in BLEU and CodeBLEU over base model outputs.</li>
        <li>Built a JSONL dataset and implemented loss masking and token alignment optimization for cleaner completions.</li>
        <li>Presented at Boise State’s 2025 Undergraduate Research Showcase; awarded the <strong>Excellence in Undergraduate Research and Creative Activity Award</strong>.</li>
        <li><a href="https://github.com/laumiulun/Sabcor" class="btn-link" target="_blank">View GitHub</a> |
            <a href="https://scholarworks.boisestate.edu/under_showcase_2025/159/" class="btn-link" target="_blank">View ScholarWorks</a></li>
      </ul>
    </div>

    <!-- Project 2 -->
    <div class="chat-bubble">
      <h2>Grounded Semantics with CLIP & Word Classifiers</h2>
      <ul>
        <li>Used OpenAI's CLIP model to map image–text pairings to shared embeddings.</li>
        <li>Built binary classifiers to determine word presence from visual input, exploring multimodal language understanding.</li>
        <li>Analyzed learned concept associations using synthetic and real-world image datasets.</li>
      </ul>
    </div>

    <!-- Project 3 -->
    <div class="chat-bubble">
      <h2>Evaluation of Pretrained LLMs on Programming Language Tasks</h2>
      <ul>
        <li>Benchmarked multiple LLMs (StarCoder, CodeGen, GPT-2) on scientific code transformation tasks.</li>
        <li>Used BLEU, CodeBLEU, and token-level accuracy as evaluation metrics.</li>
        <li>Studied effects of prompt leakage and token alignment in few-shot code translation settings.</li>
      </ul>
    </div>

    <!-- Certificates and Related Highlights -->
    <h2>Certifications & Skills</h2>
    <div class="chat-bubble">
      <ul>
        <li><strong>Generative Language AI Certificate</strong> – Boise State University</li>
        <li>Fine-tuning, pretraining, prompt design, tokenization, loss masking</li>
        <li>Tools: Hugging Face Transformers, PyTorch, CLIP, StarCoder, CodeBLEU, NLTK, Scikit-learn</li>
      </ul>
    </div>
  </section>
</body>
</html>
