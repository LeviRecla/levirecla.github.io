<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="description" content="NLP Projects by Levi Recla" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>NLP Projects - Levi Recla</title>
  <link
    href="https://fonts.googleapis.com/css2?family=Inter:wght@300;500;700&display=swap"
    rel="stylesheet"
  />
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <header class="header-bar">
    <div class="header-logo">
      <a href="index.html">Levi Recla</a>
    </div>
    <nav class="header-nav">
      <a href="about.html">About</a>
      <a href="education.html">Education</a>
      <a href="experience.html">Experience</a>
      <a href="projects.html">Projects</a>
      <a href="nlp.html">NLP</a>
      <a href="skills.html">Skills</a>
      <a href="contact.html">Contact</a>
    </nav>
  </header>

  <section class="content-section">
    <h1>Natural Language Processing (NLP)</h1>
  
    <div class="chat-bubble">
      <p>
        My work in NLP explores the intersection of language, code, and scientific computation.
        I focus on applying modern language models to automate programming language translation, understand grounded semantics, and improve scientific data interpretation.
        This page highlights key NLP projects, research contributions, and model development work.
      </p>
    </div>
  
    <!-- LANGUAGE MODEL PRETRAINING -->
    <div class="language-section">
      <h2><i class="fas fa-robot"></i> Language Model Pretraining</h2>
      <div class="chat-bubble">
        <h3>Pretraining a Transformer Language Model from Scratch</h3>
        <ul>
          <li>Trained a transformer model on a raw code corpus using a tokenizer built from scratch with Hugging Face’s <code>tokenizers</code> library.</li>
          <li>Used causal language modeling (AutoModelForCausalLM) and Hugging Face’s <code>Trainer</code> API to manage training and checkpointing.</li>
          <li>Tracked loss across epochs and validated training integrity through periodic evaluation and model saving.</li>
          <li>
            <a href="https://github.com/LeviRecla/Language-Model-Pre-Training" target="_blank" class="github-button">
              <i class="fas fa-file-code"></i> View Notebook
            </a>
          </li>
        </ul>
      </div>
    </div>
  
    <!-- BERT FINE-TUNING -->
    <div class="language-section">
      <h2><i class="fas fa-brain"></i> BERT Fine-Tuning for NLP Tasks</h2>
      <div class="chat-bubble">
        <h3>Fine-Tuning BERT for Sentence-Level Classification</h3>
        <ul>
          <li>Fine-tuned a BERT-based model using Hugging Face Transformers for binary sentence classification.</li>
          <li>Applied token-level attention masking and added a classification head on top of the pooled output from BERT.</li>
          <li>Evaluated model performance using precision, recall, F1-score, and confusion matrix visualization.</li>
          <li>Utilized `Trainer`, `DataCollatorWithPadding`, and batch encoders for efficient training and evaluation.</li>
        </ul>
      </div>
    </div>
  
    <!-- FORTRAN-TO-PYTHON FINE-TUNING -->
    <div class="chat-bubble">
      <h2><i class="fas fa-robot"></i> Language Model Fine-Tuning</h2>
      <h3>Adapting StarCoder2 for Fortran-to-Python Code Translation</h3>
      <ul>
        <li>Fine-tuned StarCoder2-3B using LoRA on a dataset of 91 manually aligned Fortran–Python code pairs.</li>
        <li>Achieved BLEU score improvement from 0.0731 to 0.2364, significantly improving translation of legacy scientific code.</li>
        <li>Applied label masking to ignore prompt loss and enforced structural consistency in completions.</li>
        <li>Presented results at Boise State's Undergraduate Research Showcase and received the <strong>Excellence in Undergraduate Research and Creative Activity Award</strong>.</li>
        <li>
          <a href="https://github.com/laumiulun/Sabcor" class="btn-link" target="_blank">View GitHub</a> |
          <a href="https://scholarworks.boisestate.edu/under_showcase_2025/159/" class="btn-link" target="_blank">View ScholarWorks</a>
        </li>
      </ul>
    </div>
  
    <!-- GROUNDED SEMANTICS -->
    <div class="chat-bubble">
      <h2><i class="fas fa-eye"></i> Grounded Semantics with CLIP</h2>
      <ul>
        <li>Used CLIP to align image-text pairs into a shared latent space.</li>
        <li>Trained binary classifiers to detect presence of words in image descriptions, leveraging embeddings for grounded reasoning.</li>
        <li>Visualized attention-based concept relationships between visual features and linguistic categories.</li>
      </ul>
    </div>
  
    <!-- BENCHMARKING MODELS -->
    <div class="chat-bubble">
      <h2><i class="fas fa-code-branch"></i> Model Evaluation on Code Tasks</h2>
      <ul>
        <li>Benchmarked StarCoder2, GPT-2, and CodeGen on code translation and generation benchmarks.</li>
        <li>Compared model outputs using BLEU, CodeBLEU, and structural token-level accuracy.</li>
        <li>Identified weaknesses in prompt leakage and tokenization inconsistencies across few-shot translation settings.</li>
      </ul>
    </div>
  
    <!-- CERTS -->
    <h2><i class="fas fa-certificate"></i> Certifications and Tools</h2>
    <div class="chat-bubble">
      <ul>
        <li><strong>Generative Language AI Certificate</strong> – Boise State University</li>
        <li><strong>Data Science for the Sciences Certificate</strong></li>
        <li>Hugging Face Transformers, Tokenizers, PyTorch, CLIP, Scikit-learn, CodeBLEU, NLTK</li>
      </ul>
    </div>
  </section>
  
</body>
</html>
